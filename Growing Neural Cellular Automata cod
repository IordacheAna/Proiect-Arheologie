#@title Imports and Notebook Utilities
%tensorflow_version 2.x
import os
import io
import PIL.Image, PIL.ImageDraw
import base64
import zipfile
import json
import requests
import numpy as np
import matplotlib.pylab as pl
import glob
import tensorflow as tf
from IPython.display import Image, HTML, clear_output
import tqdm
import os
os.environ['FFMPEG_BINARY'] = 'ffmpeg'
import moviepy.editor as mvp
from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter
clear_output()
# Converteste un numpy array 칥ntr-un obiect PIL Image
def np2pil(a):
  if a.dtype in [np.float32, np.float64]:
    a = np.uint8(np.clip(a, 0, 1)*255)
  return PIL.Image.fromarray(a)
# Scrie un numpy array ca imagine 칥ntr-un fi탳ier.
def imwrite(f, a, fmt=None):
  a = np.asarray(a)
  if isinstance(f, str):
    fmt = f.rsplit('.', 1)[-1].lower()
    if fmt == 'jpg':
      fmt = 'jpeg'
    f = open(f, 'wb')
  np2pil(a).save(f, fmt, quality=95)
# Encodeaz캒 un numpy array 칥ntr-un format de imagine specificat 탳i returneaz캒 datele codate.
def imencode(a, fmt='jpeg'):
  a = np.asarray(a)
  if len(a.shape) == 3 and a.shape[-1] == 4:
    fmt = 'png'
  f = io.BytesIO()
  imwrite(f, a, fmt)
  return f.getvalue()
# Converteste un numpy array 칥ntr-un URL de imagine codat캒 칥n base64, util pentru afi탳area inline 칥n notebook-uri.
def im2url(a, fmt='jpeg'):
  encoded = imencode(a, fmt)
  base64_byte_string = base64.b64encode(encoded).decode('ascii')
  return 'data:image/' + fmt.upper() + ';base64,' + base64_byte_string
# Afiseaz캒 un numpy array ca imagine 칥n notebook.
def imshow(a, fmt='jpeg'):
  display(Image(data=imencode(a, fmt)))
# Aranjeaz캒 un set de imagini (reprezentate ca numpy arrays) 칥ntr-o singur캒 imagine 칥n form캒 de 탵igl캒.
def tile2d(a, w=None):
  a = np.asarray(a)
  if w is None:
    w = int(np.ceil(np.sqrt(len(a))))
  th, tw = a.shape[1:3]
  pad = (w-len(a))%w
  a = np.pad(a, [(0, pad)]+[(0, 0)]*(a.ndim-1), 'constant')
  h = len(a)//w
  a = a.reshape([h, w]+list(a.shape[1:]))
  a = np.rollaxis(a, 2, 1).reshape([th*h, tw*w]+list(a.shape[4:]))
  return a
# M캒re탳te o imagine printr-o replicare simpl캒 a pixelilor.
def zoom(img, scale=4):
  img = np.repeat(img, scale, 0)
  img = np.repeat(img, scale, 1)
  return img

class VideoWriter:
  def __init__(self, filename, fps=30.0, **kw):
    self.writer = None
    self.params = dict(filename=filename, fps=fps, **kw)
# Adauga consecutiv mai multe cadre la videoclip, gestion칙nd automat detaliile de formatare 탳i scriere necesare pentru crearea videoclipului.
  def add(self, img):
    img = np.asarray(img)
    if self.writer is None:
      h, w = img.shape[:2]
      self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)
    if img.dtype in [np.float32, np.float64]:
      img = np.uint8(img.clip(0, 1)*255)
    if len(img.shape) == 2:
      img = np.repeat(img[..., None], 3, -1)
    self.writer.write_frame(img)

  def close(self):
    if self.writer:
      self.writer.close()

  def __enter__(self):
    return self

  def __exit__(self, *kw):
    self.close()

#@title Cellular Automata Parameters
# Num캒rul de canale de stare pentru automatele celulare. Acesta poate fi interpretat ca profunzimea informa탵iei de stare pe care fiecare celul캒 o poate de탵ine.
CHANNEL_N = 16        
# Num캒rul de pixeli folosi탵i pentru a ad캒uga o margine 칥n jurul imaginii 탵int캒. Aceasta este util캒 pentru a preveni efectele de margine 칥n timpul evolu탵iei automatei celulare.
TARGET_PADDING = 16   
# Dimensiunea 탵intei (칥n pixeli), specific칙nd dimensiunea la care imaginea 탵int캒 va fi scalat캒.
TARGET_SIZE = 40
# M캒rimea lotului pentru antrenament, indic칙nd c칙te instan탵e sunt procesate 칥mpreun캒 칥ntr-o itera탵ie de antrenament.
BATCH_SIZE = 8
# Dimensiunea pool-ului de modele sau de st캒ri ini탵iale din care se pot selecta instan탵e pentru antrenament sau evolu탵ie.
POOL_SIZE = 1024
# Rata la care celulele din automata celular캒 se "activeaz캒" sau schimb캒 starea 칥ntr-o itera탵ie. Aceasta este un parametru care influen탵eaz캒 dinamica evolu탵iei automatei celulare.
CELL_FIRE_RATE = 0.5
# Un emoji folosit ca 탵int캒 vizual캒 pentru experiment.
TARGET_EMOJI = "游붍" #@param {type:"string"}
# Tipul de experiment efectuat cu automata celular캒. Poate fi "Growing" (Cresc캒tor), "Persistent" (Persistent) sau "Regenerating" (Regenerativ).
EXPERIMENT_TYPE = "Regenerating" #@param ["Growing", "Persistent", "Regenerating"]
# Un dic탵ionar care mapeaz캒 numele tipului de experiment la un index numeric.
EXPERIMENT_MAP = {"Growing":0, "Persistent":1, "Regenerating":2}
EXPERIMENT_N = EXPERIMENT_MAP[EXPERIMENT_TYPE]
# Un indicator boolean (0 sau 1) derivat din tipul de experiment, care decide dac캒 se va folosi un pool de modele/pattern-uri pentru antrenament.
USE_PATTERN_POOL = [0, 1, 1][EXPERIMENT_N]
# Num캒rul de pattern-uri dintr-un lot care vor fi "deteriorate" sau modificate 칥n timpul experimentului, bazat pe tipul de experiment. 
#Acest lucru este relevant pentru experimentele regenerative, unde anumite pattern-uri sunt inten탵ionat deteriorate pentru a testa capacitatea de regenerare a sistemului.
DAMAGE_N = [0, 0, 3][EXPERIMENT_N] 

#@title CA Model and Utilities

from tensorflow.keras.layers import Conv2D
# 칉ncarc캒 o imagine de la un URL dat, redimension칙nd-o la o dimensiune maxim캒 specificat캒 탳i normaliz칙nd valorile pixelilor la intervalul [0, 1].
def load_image(url, max_size=TARGET_SIZE):
  r = requests.get(url)
  img = PIL.Image.open(io.BytesIO(r.content))
  img.thumbnail((max_size, max_size), PIL.Image.ANTIALIAS)
  img = np.float32(img)/255.0
  img[..., :3] *= img[..., 3:]
  return img
# 칉ncarc캒 o imagine emoji de pe GitHub folosind codul s캒u unic, apel칙nd load_image pentru a procesa 탳i returna emoji-ul ca o imagine.
def load_emoji(emoji):
  code = hex(ord(emoji))[2:].lower()
  url = 'https://github.com/googlefonts/noto-emoji/blob/main/png/128/emoji_u%s.png?raw=true'%code
  return load_image(url)

#  Func탵ia returneaz캒 un tensor cu aceste patru canale, reprezent칙nd imaginea 칥n format RGBA (unde canalul Alfa indic캒 transparen탵a).
def to_rgba(x):
  return x[..., :4]
# Aceast캒 func탵ie extrage canalul alfa (transparen탵a) din tensor. 
# Folose탳te tf.clip_by_value pentru a asigura c캒 valorile canalului alfa sunt 칥ntre 0.0 (complet transparent) 탳i 1.0 (complet opac). 
# Aceast캒 opera탵ie este util캒 pentru a manipula sau a analiza transparen탵a imaginilor 칥n procesele care implic캒 imagini cu canal alfa.
def to_alpha(x):
  return tf.clip_by_value(x[..., 3:4], 0.0, 1.0)
# Returneaza o reprezentare RGB pur캒 a imaginii, f캒r캒 a 탵ine cont de transparen탵캒.
def to_rgb(x):
  rgb, a = x[..., :3], to_alpha(x)
  return 1.0-a+rgb
#  Determin캒 "celulele vii" dintr-o stare CA, baz칙ndu-se pe transparen탵a (alpha) celulelor. 
def get_living_mask(x):
  alpha = x[:, :, :, 3:4]
  return tf.nn.max_pool2d(alpha, 3, [1, 1, 1, 1], 'SAME') > 0.1
# Creeaz캒  o stare ini탵ial캒 concentrat캒 칥n centrul unui tensor, cu restul tensorului fiind gol.
def make_seed(size, n=1):
  x = np.zeros([n, size, size, CHANNEL_N], np.float32)
  x[:, size//2, size//2, 3:] = 1.0
  return x


class CAModel(tf.keras.Model):
# Ini탵ializeaz캒 modelul cu un num캒r specific de canale 탳i o rat캒 de "aprindere" (fire rate), care influen탵eaz캒 probabilitatea fiec캒rei celule de a se actualiza la fiecare pas de timp. 
#Include 탳i definirea unui model secven탵ial (dmodel) cu dou캒 straturi convolu탵ionale, destinat proces캒rii percep탵iilor celulelor.
  def __init__(self, channel_n=CHANNEL_N, fire_rate=CELL_FIRE_RATE):
    super().__init__()
    self.channel_n = channel_n
    self.fire_rate = fire_rate

    self.dmodel = tf.keras.Sequential([
          Conv2D(128, 1, activation=tf.nn.relu),
          Conv2D(self.channel_n, 1, activation=None,
              kernel_initializer=tf.zeros_initializer),
    ])

    self(tf.zeros([1, 3, 3, channel_n]))
#Implementeaz캒 percep탵ia pentru celule, folosind filtre de convolu탵ie pentru a capta informa탵ii spa탵iale din vecin캒tatea fiec캒rei celule. 
#Acest mecanism permite celulei s캒 "simt캒" starea 칥nconjur캒toare 탳i s캒 reac탵ioneze 칥n consecin탵캒.
  @tf.function
  def perceive(self, x, angle=0.0):
    identify = np.float32([0, 1, 0])
    identify = np.outer(identify, identify)
    dx = np.outer([1, 2, 1], [-1, 0, 1]) / 8.0 
    dy = dx.T
    c, s = tf.cos(angle), tf.sin(angle)
    kernel = tf.stack([identify, c*dx-s*dy, s*dx+c*dy], -1)[:, :, None, :]
    kernel = tf.repeat(kernel, self.channel_n, 2)
    y = tf.nn.depthwise_conv2d(x, kernel, [1, 1, 1, 1], 'SAME')
    return y
# Metoda principal캒 de actualizare a st캒rii automatei celulare. 
# Determin캒 celulele care r캒m칙n "vii" 칥nainte 탳i dup캒 actualizare, aplic캒 perceputul 탳i actualizeaz캒 starea 칥n func탵ie de rata de aprindere 탳i masca de actualizare. 
# Folose탳te perceive pentru a ob탵ine informa탵iile necesare actualiz캒rii 탳i dmodel pentru a calcula schimb캒rile de stare.
  @tf.function
  def call(self, x, fire_rate=None, angle=0.0, step_size=1.0):
    pre_life_mask = get_living_mask(x)

    y = self.perceive(x, angle)
    dx = self.dmodel(y)*step_size
    if fire_rate is None:
      fire_rate = self.fire_rate
    update_mask = tf.random.uniform(tf.shape(x[:, :, :, :1])) <= fire_rate
    x += dx * tf.cast(update_mask, tf.float32)

    post_life_mask = get_living_mask(x)
    life_mask = pre_life_mask & post_life_mask
    return x * tf.cast(life_mask, tf.float32)


CAModel().dmodel.summary()

# Training

#@title Train Utilities (SamplePool, Model Export, Damage)
from google.protobuf.json_format import MessageToDict
from tensorflow.python.framework import convert_to_constants

class SamplePool:
# Ini탵ializeaz캒 un pool de exemple cu datele furnizate prin argumentele cu nume. 
# Fiecare argument cu nume reprezint캒 un slot de date (de exemplu, st캒ri ale automatei celulare), cu valorile fiind liste sau numpy arrays.
  def __init__(self, *, _parent=None, _parent_idx=None, **slots):
    self._parent = _parent
    self._parent_idx = _parent_idx
    self._slot_names = slots.keys()
    self._size = None
    for k, v in slots.items():
      if self._size is None:
        self._size = len(v)
      assert self._size == len(v)
      setattr(self, k, np.asarray(v))
# Extrage un subset aleatoriu din pool, form칙nd un nou SamplePool cu aceste date. 
# Acest mecanism este folosit pentru a selecta un batch de exemple pentru antrenare.
  def sample(self, n):
    idx = np.random.choice(self._size, n, False)
    batch = {k: getattr(self, k)[idx] for k in self._slot_names}
    batch = SamplePool(**batch, _parent=self, _parent_idx=idx)
    return batch
# Actualizeaz캒 datele 칥n pool-ul p캒rinte cu modific캒rile efectuate 칥n subsetul extras prin sample. 
# Aceasta este util캒 pentru a "칥nv캒탵a" din modific캒rile aplicate pe exemplele antrenate.
  def commit(self):
    for k in self._slot_names:
      getattr(self._parent, k)[self._parent_idx] = getattr(self, k)
# Genereaz캒 m캒탳ti circulare 칥ntr-un tensor, folosite pentru a aplica "daune" asupra st캒rilor automatei celulare 칥n experimentele de regenerare.
@tf.function
def make_circle_masks(n, h, w):
  x = tf.linspace(-1.0, 1.0, w)[None, None, :]
  y = tf.linspace(-1.0, 1.0, h)[None, :, None]
  center = tf.random.uniform([2, n, 1, 1], -0.5, 0.5)
  r = tf.random.uniform([n, 1, 1], 0.1, 0.4)
  x, y = (x-center[0])/r, (y-center[1])/r
  mask = tf.cast(x*x+y*y < 1.0, tf.float32)
  return mask
# Export캒 modelul automat캒 celular캒 칥ntr-un format care poate fi salvat 탳i 칥nc캒rcat ulterior.
def export_model(ca, base_fn):
  ca.save_weights(base_fn)

  cf = ca.call.get_concrete_function(
      x=tf.TensorSpec([None, None, None, CHANNEL_N]),
      fire_rate=tf.constant(0.5),
      angle=tf.constant(0.0),
      step_size=tf.constant(1.0))
  cf = convert_to_constants.convert_variables_to_constants_v2(cf)
  graph_def = cf.graph.as_graph_def()
  graph_json = MessageToDict(graph_def)
  graph_json['versions'] = dict(producer='1.14', minConsumer='1.14')
  model_json = {
      'format': 'graph-model',
      'modelTopology': graph_json,
      'weightsManifest': [],
  }
  with open(base_fn+'.json', 'w') as f:
    json.dump(model_json, f)
# Genereaz캒 o imagine care arat캒 starea actual캒 a unui subset din pool-ul de exemple 탳i salveaz캒 aceast캒 imagine 칥ntr-un fi탳ier.
def generate_pool_figures(pool, step_i):
  tiled_pool = tile2d(to_rgb(pool.x[:49]))
  fade = np.linspace(1.0, 0.0, 72)
  ones = np.ones(72) 
  tiled_pool[:, :72] += (-tiled_pool[:, :72] + ones[None, :, None]) * fade[None, :, None] 
  tiled_pool[:, -72:] += (-tiled_pool[:, -72:] + ones[None, :, None]) * fade[None, ::-1, None]
  tiled_pool[:72, :] += (-tiled_pool[:72, :] + ones[:, None, None]) * fade[:, None, None]
  tiled_pool[-72:, :] += (-tiled_pool[-72:, :] + ones[:, None, None]) * fade[::-1, None, None]
  imwrite('train_log/%04d_pool.jpg'%step_i, tiled_pool)
# Aceast캒 func탵ie creeaz캒 탳i salveaz캒 o imagine care compar캒 st캒rile ini탵iale ale unui batch de exemple (x0) cu st캒rile lor dup캒 un pas de antrenare (x). 
def visualize_batch(x0, x, step_i):
  vis0 = np.hstack(to_rgb(x0).numpy())
  vis1 = np.hstack(to_rgb(x).numpy())
  vis = np.vstack([vis0, vis1])
  imwrite('train_log/batches_%04d.jpg'%step_i, vis)
  print('batch (before/after):')
  imshow(vis)
# Afi탳eaz캒 un grafic al istoricului de pierderi (loss) pe parcursul antren캒rii, ajut칙nd la evaluarea convergen탵ei modelului.
def plot_loss(loss_log):
  pl.figure(figsize=(10, 4))
  pl.title('Loss history (log10)')
  pl.plot(np.log10(loss_log), '.', alpha=0.1)
  pl.show()
# Aceast캒 func탵ie converte탳te emoji-ul specificat 칥ntr-o imagine.
target_img = load_emoji(TARGET_EMOJI)
imshow(zoom(to_rgb(target_img), 2), fmt='png')
# pad_target: Imaginea 탵int캒 este extins캒 ad캒ug칙nd un padding (margine) de TARGET_PADDING pixeli pe toate laturile, cu excep탵ia canalelor de culoare. 
# Acest lucru este f캒cut pentru a permite automatei celulare s캒 creasc캒 spre imaginea 탵int캒 f캒r캒 a fi restric탵ionat캒 de marginile ini탵iale ale imaginii.
p = TARGET_PADDING
pad_target = tf.pad(target_img, [(p, p), (p, p), (0, 0)])
h, w = pad_target.shape[:2]
seed = np.zeros([h, w, CHANNEL_N], np.float32)
seed[h//2, w//2, 3:] = 1.0
# : Defineste func탵ia de pierdere ca fiind media p캒tratelor diferen탵elor 칥ntre starea actual캒 a automatei celulare (convertit캒 la format RGBA) 탳i imaginea 탵int캒 cu padding.
# Aceast캒 func탵ie de pierdere m캒soar캒 c칙t de bine reproduce automatul celular imaginea 탵int캒, ghid칙nd ajust캒rile parametrilor modelului 칥n timpul antrenamentului.
def loss_f(x):
  return tf.reduce_mean(tf.square(to_rgba(x)-pad_target), [-2, -3, -1])

ca = CAModel()

loss_log = []
#  Seteaz캒 un optimizator Adam cu un program de rat캒 de 칥nv캒탵are ajustabil.
lr = 2e-3
lr_sched = tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    [2000], [lr, lr*0.1])
trainer = tf.keras.optimizers.Adam(lr_sched)
#  Creeaz캒 un pool de exemple pentru antrenament, ini탵ializ칙nd fiecare exemplu cu starea "seed" definit캒 mai sus. 
loss0 = loss_f(seed).numpy()
pool = SamplePool(x=np.repeat(seed[None, ...], POOL_SIZE, 0))

# Se preg캒te탳te un director pentru a stoca loguri 탳i output-uri vizuale ale antrenamentului, asigur칙ndu-se c캒 este gol la 칥nceputul antrenamentului.
!mkdir -p train_log && rm -f train_log/*

# train_step este un mecanism prin care modelul 칥nva탵캒 s캒 reproduc캒 imaginea 탵int캒, ajust칙nd iterativ ponderile pe baza feedback-ului de pierdere, 
#cu scopul de a minimiza diferen탵a 칥ntre output-ul modelului 탳i imaginea 탵int캒. 
@tf.function
def train_step(x):
  iter_n = tf.random.uniform([], 64, 96, tf.int32)
# GradientTape monitorizeaz캒 opera탵iile pentru a determina gradientul func탵iei de pierdere fa탵캒 de ponderile modelului.
  with tf.GradientTape() as g:
    for i in tf.range(iter_n):
      x = ca(x)
    loss = tf.reduce_mean(loss_f(x))
  grads = g.gradient(loss, ca.weights)
  grads = [g/(tf.norm(g)+1e-8) for g in grads]
  trainer.apply_gradients(zip(grads, ca.weights))
  return x, loss
#Dureaza foarte mult(20 de minute) sa compileze pana la 8000. Se poate schimba pana la 500
# Func탵ia train_step este apelat캒 cu batch-ul de exemple selectat pentru a ajusta ponderile modelului.
# aceast캒 bucl캒 este conceput캒 pentru a optimiza modelul CA prin ajust캒ri iterative ale ponderilor pe baza feedback-ului de pierdere, 
# utiliz칙nd un set variabil de exemple pentru a 칥mbun캒t캒탵i capacitatea modelului de a reproduce sau a regenera imaginea 탵int캒, 
# 탳i pentru a testa robuste탵ea acestuia la "daune"........

for i in range(8000+1):
  if USE_PATTERN_POOL:
    batch = pool.sample(BATCH_SIZE)
    x0 = batch.x
    loss_rank = loss_f(x0).numpy().argsort()[::-1]
    x0 = x0[loss_rank]
    x0[:1] = seed
    if DAMAGE_N:
      damage = 1.0-make_circle_masks(DAMAGE_N, h, w).numpy()[..., None]
      x0[-DAMAGE_N:] *= damage
  else:
    x0 = np.repeat(seed[None, ...], BATCH_SIZE, 0)

  x, loss = train_step(x0)

  if USE_PATTERN_POOL:
    batch.x[:] = x
    batch.commit()

  step_i = len(loss_log)
  loss_log.append(loss.numpy())
  
  if step_i%10 == 0:
    generate_pool_figures(pool, step_i)
  if step_i%100 == 0:
    clear_output()
    visualize_batch(x0, x, step_i)
    plot_loss(loss_log)
    export_model(ca, 'train_log/%04d'%step_i)

  print('\r step: %d, log10(loss): %.3f'%(len(loss_log), np.log10(loss)), end='')

# Figures

#@title Training Progress (Checkpoints)

models = []
#Trebuie schimbat pana in maxim 500...
#for i in [100, 200, 300, 400]
for i in [100, 500, 1000, 4000]:
  ca = CAModel()
  ca.load_weights('train_log/%04d'%i)
  models.append(ca)

# Se preg캒te탳te un tensor x care va stoca st캒rile ini탵iale pentru fiecare model.
out_fn = 'train_steps_damage_%d.mp4'%DAMAGE_N
x = np.zeros([len(models), 72, 72, CHANNEL_N], np.float32)
x[..., 36, 36, 3:] = 1.0
# Se folose탳te o bucl캒 for pentru a simula 500 de pa탳i de evolu탵ie. La fiecare pas, 
# se genereaz캒 o vizualizare a st캒rii curente pentru fiecare model prin concatenarea orizontal캒 a st캒rilor (transformate 칥n RGB) 
# 탳i ad캒ugarea rezultatului 칥n videoclip. 
with VideoWriter(out_fn) as vid:
  for i in tqdm.trange(500):
    vis = np.hstack(to_rgb(x))
    vid.add(zoom(vis, 2))
    for ca, xk in zip(models, x):
      xk[:] = ca(xk[None,...])[0]
mvp.ipython_display(out_fn)

# genereaz캒 탳i afi탳eaz캒 un videoclip care ilustreaz캒 progresul antrenamentului modelului de automate celulare (CA) pe parcursul mai multor batch-uri de date.
frames = sorted(glob.glob('train_log/batches_*.jpg'))
mvp.ImageSequenceClip(frames, fps=10.0).write_videofile('batches.mp4')
mvp.ipython_display('batches.mp4')

# Aceast캒 sec탵iune de cod creeaz캒 탳i afi탳eaz캒 un videoclip care ilustreaz캒 con탵inutul pool-ului de exemple pe parcursul antrenamentului unui model de automate celulare (CA).
# Este similar cu generarea videoclipului pentru batch-uri, dar se concentreaz캒 칥n schimb pe a ar캒ta evolu탵ia pool-ului de exemple.
frames = sorted(glob.glob('train_log/*_pool.jpg'))[:80]
mvp.ImageSequenceClip(frames, fps=20.0).write_videofile('pool.mp4')
mvp.ipython_display('pool.mp4')

## Pretrained Models and Figures

# Descarca un set de modele preantrenate de automate celulare (CA) de pe GitHub, le dezarhiveaza local, 
# 탳i apoi s캒 furnizeaza o func탵ie pentru a 칥nc캒rca un model specific bazat pe parametrii da탵i (fire_rate, use_sample_pool, damane_n)
!wget -O models.zip 'https://github.com/google-research/self-organising-systems/blob/master/assets/growing_ca/models.zip?raw=true'
!unzip -oq models.zip

EMOJI = '游붍游游눤游녜游멇릱游륋릣젏린뻟릝'
# faciliteaz캒 칥nc캒rcarea rapid캒 탳i u탳oar캒 a unui model preantrenat de automate celulare pentru utilizare ulterioar캒
def get_model(emoji='游붊', fire_rate=0.5, use_pool=1, damage_n=3, run=0,
              prefix='models/', output='model'):
  path = prefix
  assert fire_rate in [0.5, 1.0]
  if fire_rate==0.5:
    path += 'use_sample_pool_%d damage_n_%d '%(use_pool, damage_n)
  elif fire_rate==1.0:
    path += 'fire_rate_1.0 '
  code = hex(ord(emoji))[2:].upper()
  path += 'target_emoji_%s run_index_%d/08000'%(code, run)
  assert output in ['model', 'json']
  if output == 'model':
    ca = CAModel(channel_n=16, fire_rate=fire_rate)
    ca.load_weights(path)
    return ca
  elif output == 'json':
    return open(path+'.json', 'r').read()
#  Acest cod creeaz캒 탳i afi탳eaz캒 o imagine compus캒 prin concatenarea orizontal캒 (folosind np.hstack) a imaginilor reprezent칙nd emoji-uri individuale
atlas = np.hstack([load_emoji(e) for e in EMOJI])
imshow(atlas)

#@title Teaser
# Creeaz캒 un "teaser" video care demonstreaz캒 capacitatea mai multor modele de automate celulare (CA) de a cre탳te, a suferi daune 탳i apoi de a se regenera, 
# utiliz칙nd o serie de emoji-uri ca 탵inte pentru antrenament. Procesul este 칥mp캒r탵it 칥n trei faze principale: cre탳tere, daune 탳i estompare (fade out).

# Fiecare model este antrenat pentru a reproduce forma specific캒 a emoji-ului s캒u 탵int캒.
models = [get_model(emoji, run=1) for emoji in EMOJI]

with VideoWriter('teaser.mp4') as vid:
  x = np.zeros([len(EMOJI), 64, 64, CHANNEL_N], np.float32)
  # grow
  # 칉n primele 200 de cadre, modelele sunt actualizate iterativ, permi탵칙nd fiec캒rui model s캒 creasc캒 de la punctul s캒u central activat.
  for i in tqdm.trange(200):
    k = i//20
    if i%20==0 and k<len(EMOJI):
      x[k, 32, 32, 3:] = 1.0
    vid.add(zoom(tile2d(to_rgb(x), 5), 2))
    for ca, xk in zip(models, x):
      xk[:] = ca(xk[None,...])[0]
  # damage
  # Apoi, se aplic캒 "daune" modelelor prin crearea unei m캒탳ti care simuleaz캒 trecerea unui obiect prin st캒rile modelelor, erod칙ndu-le.
  mask = PIL.Image.new('L', (64*5, 64*2))
  draw = PIL.ImageDraw.Draw(mask)
  for i in tqdm.trange(400):
    cx, r = i*3-20, 6
    y1, y2 = 32+np.sin(i/5+np.pi)*8, 32+64+np.sin(i/5)*8
    draw.rectangle((0, 0, 64*5, 64*2), fill=0)
    draw.ellipse((cx-r, y1-r, cx+r, y1+r), fill=255)
    draw.ellipse((cx-r, y2-r, cx+r, y2+r), fill=255)
    x *= 1.0-(np.float32(mask).reshape(2, 64, 5, 64)
        .transpose([0, 2, 1, 3]).reshape(10, 64, 64, 1))/255.0
    if i<200 or i%2 == 0:
      vid.add(zoom(tile2d(to_rgb(x), 5), 2))
    for ca, xk in zip(models, x):
      xk[:] = ca(xk[None,...])[0]
  # fade out
  # Videoclipul se 칥ncheie cu o estompare gradual캒 a ultimei imagini.
  last = zoom(tile2d(to_rgb(x), 5), 2)
  for t in np.linspace(0, 1, 30):
    vid.add(last*(1.0-t)+t)

mvp.ipython_display('teaser.mp4', loop=True)

#@title Unstable Patterns
# Se creeaz캒 un videoclip care demonstreaz캒 comportamentul unor modele de automate celulare (CA) antrenate pentru a reproduce diferite emoji-uri, 
#concentr칙ndu-se pe manifestarea unor modele instabile sau dinamice pe parcursul evolu탵iei. 
# Videoclipul include, de asemenea, o reprezentare vizual캒 a unui "slider" pentru a indica progresul temporal al simul캒rii.
!wget -O slider.png 'https://github.com/google-research/self-organising-systems/raw/master/assets/growing_ca/slider.png?raw=true'

import PIL.ImageFont
from matplotlib import font_manager as fm
font_fn = fm.findfont(fm.FontProperties())
font = PIL.ImageFont.truetype(font_fn, 20)
# Se creeaz캒 o list캒 de modele CA, c칙te unul pentru fiecare emoji din 탳irul EMOJI, cu use_pool=0 탳i damage_n=0, 
# indic칙nd c캒 modelele sunt ini탵ializate f캒r캒 a utiliza un pool de exemple 탳i f캒r캒 a aplica daune.
models = [get_model(ch, use_pool=0, damage_n=0) for ch in EMOJI]
fn = 'unstable.mp4'
with VideoWriter(fn) as vid:
  x = np.zeros([len(EMOJI), 64, 64, CHANNEL_N], np.float32)
  x[:, 32, 32, 3:] = 1.0
  # grow
  #  Modelele sunt actualizate iterativ pentru 1000 de pa탳i. La fiecare pas relevant (primele 200 de pa탳i 탳i apoi la fiecare al 5-lea pas), 
  # se genereaz캒 o imagine a st캒rilor actuale ale modelelor, se adaug캒 reprezentarea slider-ului 탳i se indic캒 progresul simul캒rii prin plasarea unui marcator pe slider.
  slider = PIL.Image.open("slider.png")
  for i in tqdm.trange(1000):
    if i<200 or i%5 == 0:
      vis = zoom(tile2d(to_rgb(x), 5), 4).clip(0, 1)
      vis_extended = np.concatenate((vis, np.ones((164, vis.shape[1], 3))), axis=0) 
      im = np.uint8(vis_extended*255)
      im = PIL.Image.fromarray(im)
      im.paste(slider, box=(20, vis.shape[0]+20))
      draw = PIL.ImageDraw.Draw(im)
      p_x = (14 + (610/1000)*i)*2.0
      draw.rectangle([p_x, vis.shape[0]+20+55, p_x+10, vis.shape[0]+20+82], fill="#434343bd")
      vid.add(np.uint8(im))
    for ca, xk in zip(models, x):
      xk[:] = ca(xk[None,...])[0]
  # fade out
  for t in np.linspace(0, 1, 30):
    vid.add(vis_extended*(1.0-t)+t)
# Videoclipul este salvat 탳i apoi afi탳at 칥n notebook folosind mvp.ipython_display, cu op탵iunea de redare 칥n bucl캒 activat캒.
mvp.ipython_display(fn, loop=True)

#@title Rotation
row_size = 4
models_of_interest = ["游붊","游붍","游","游"]
num_images = 16
imgs = []
start_angle = np.random.randint(13, 76)

for i in np.arange(num_images):
  ang = start_angle + i * np.random.randint(36, 111) # Pentru fiecare imagine din num캒rul total specificat, se calculeaz캒 un unghi de rota탵ie (ang), convertindu-l din grade 칥n radiani.
  ang = ang/360.0 * 2 * np.pi
  if i % row_size == 0:
    ca = get_model(models_of_interest[i // row_size])
  x = np.zeros([1, 56, 56, CHANNEL_N], np.float32)
  x[:, 28, 28, 3:] = 1.0
#Modelul CA este apoi aplicat pe starea ini탵ial캒 pentru 500 de itera탵ii, cu unghiul de rota탵ie specificat fiind aplicat ca parametru la fiecare pas.
#Acest proces simuleaz캒 cre탳terea sau evolu탵ia modelului sub influen탵a rota탵iei.
  for i in range(500):
    ang = tf.constant(ang, tf.float32)
    x = ca(x, angle=ang)
  imgs.append(to_rgb(x)[0])
assert len(imgs) % row_size == 0
imgs = zip(*(iter(imgs),) * row_size)
# Imaginile generate sunt adunate 칥ntr-o list캒 (imgs), care este apoi aranjat캒 칥n r칙nduri conform row_size.
# Imaginile din fiecare r칙nd sunt concatenate orizontal, iar r칙ndurile rezultate sunt concatenate vertical pentru a forma o singur캒 imagine mare (imgs_arr).
imgs_arr = np.concatenate([np.hstack(im_row) for im_row in imgs])
vis = zoom(imgs_arr, 4)

imshow(vis, fmt='png')

# demonstreaz캒 capacitatea de regenerare a unor modele de automate celulare (CA) antrenate pentru a reproduce anumite emoji-uri, 
# dar f캒r캒 a fi antrenate explicit cu scenarii de daune.
#@title Regeneration (trained without damage)
models = [get_model(ch, damage_n=0) for ch in '游游붊游붍']
with VideoWriter('regen1.mp4') as vid:
  x = np.zeros([len(models), 5, 56, 56, CHANNEL_N], np.float32)
  cx, cy = 28, 28
  x[:, :, cy, cx, 3:] = 1.0
# La pasul 200, se aplic캒 diferite tipuri de "daune" asupra st캒rilor pentru a simula situa탵ii de deteriorare: 
# t캒iere orizontal캒 탳i vertical캒, 탳i o sec탵iune central캒 p캒trat캒 este 탳tears캒, cre칙nd diverse tipuri de provoc캒ri pentru regenerare.
  for i in tqdm.trange(2000):
    if i == 200:
      x[:, 0, cy:] = x[:, 1, :cy] = 0
      x[:, 2, :, cx:] = x[:, 3, :, :cx] = 0
      x[:, 4, cy-8:cy+8, cx-8:cx+8] = 0
    vis = to_rgb(x)
    vis = np.vstack([np.hstack(row) for row in vis])
    vis = zoom(vis, 2)
    if (i < 400 and i%2==0) or i%8 == 0:
      vid.add(vis)
    if i == 200:
# Dup캒 aplicarea "daunelor", se adaug캒 o imagine static캒 pentru 29 de cadre pentru a eviden탵ia momentul daunelor.
      for _ in range(29):
        vid.add(vis)
    for ca, row in zip(models, x):
      row[:] = ca(row)

mvp.ipython_display('regen1.mp4')

#@title Regeneration (trained with damage)
# Se creeaz캒 o list캒 de modele CA, fiecare destinat s캒 reproduc캒 un anumit emoji ('游游붊游붍'), fiecare antrenat cu capacitatea de a gestiona 
# un num캒r specific de daune (damage_n=3). Aceasta 칥nseamn캒 c캒, 칥n timpul antrenamentului lor, aceste modele au fost expuse la scenarii 칥n care p캒r탵i 
# ale structurilor lor au fost eliminate, antren칙ndu-le astfel s캒 "칥nve탵e" s캒 se regenereze dup캒 astfel de daune.
models = [get_model(ch, damage_n=3) for ch in '游游붊游붍']
with VideoWriter('regen2.mp4') as vid:
  x = np.zeros([len(models), 5, 56, 56, CHANNEL_N], np.float32)
  cx, cy = 28, 28
  x[:, :, cy, cx, 3:] = 1.0
  for i in tqdm.trange(2000):
# La pasul 200, tensorul x este modificat pentru a simula diferite tipuri de daune (t캒ieturi orizontale, verticale 탳i o sec탵iune p캒trat캒 central캒 eliminat캒), 
#provoc칙nd astfel modelele s캒 demonstreze capacitatea lor de regenerare. 
    if i == 200:
      x[:, 0, cy:] = x[:, 1, :cy] = 0
      x[:, 2, :, cx:] = x[:, 3, :, :cx] = 0
      x[:, 4, cy-8:cy+8, cx-8:cx+8] = 0
    vis = to_rgb(x)
    vis = np.vstack([np.hstack(row) for row in vis])
    vis = zoom(vis, 2)
    if (i < 400 and i%2==0) or i%8 == 0:
      vid.add(vis)
    if i == 200:
#  Dup캒 aplicarea daunelor, o imagine static캒 este men탵inut캒 pentru 29 de cadre pentru a sublinia efectul daunelor.
      for _ in range(29):
        vid.add(vis)
    for ca, row in zip(models, x):
      row[:] = ca(row)

mvp.ipython_display('regen2.mp4')

#@title Planarian
# ilustreaz캒 cum s캒 generezi un videoclip ce demonstreaz캒 comportamentul unui model de automata celular캒 (CA) inspirat de planarieni
# Modelul CA este utilizat pentru a simula cre탳terea 탳i regenerarea dup캒 o "t캒iere" virtual캒, similar cu capacitatea planarienilor de a regenera p캒r탵i pierdute ale corpului lor
!wget -O planarian.zip 'https://github.com/google-research/self-organising-systems/blob/master/assets/growing_ca/planarian.zip?raw=true'
# Se descarc캒 un set de date (planarian.zip) care con탵ine ponderile antrenate pentru un model CA specificat, folosind wget. Setul de date este apoi dezarhivat 칥ntr-un director planarian.
!unzip -oq planarian.zip -d planarian

ca = CAModel()
ca.load_weights('planarian/train_log/8000')

x = np.zeros([1, 64, 96, CHANNEL_N], np.float32)
x[:, 32, 48, 3:] = 1.0
with VideoWriter('planarian.mp4', 30.0) as vid:
  for i in range(400):
    vid.add(zoom(to_rgb(x[0])))
    x = ca(x, angle=np.pi/2.0)
# La itera탵ia 150, se simuleaz캒 "t캒ierea" modelului prin efectuarea unor opera탵iuni de rotire pe por탵iuni ale st캒rii x, imit칙nd efectul unei t캒ieturi prin corpul unei planariene. 
# Acest lucru este f캒cut pentru a demonstra capacitatea modelului de a "regenera" dup캒 daune.
    if i==150:
      x = x.numpy()
      for k in range(24):
        x[:,:24] = np.roll(x[:,:24], 1, 2)
        x[:,-24:] = np.roll(x[:,-24:], -1, 2)
        vid.add(zoom(to_rgb(x[0])))
      for k in range(20):
        vid.add(zoom(to_rgb(x[0])))

mvp.ipython_display('planarian.mp4')

#@title TensorFlow.js Demo {run:"auto", vertical-output: true}
#@markdown Select "CHECKPOINT" model to load the checkpoint created by running cells from the "Training" section of this notebook
import IPython.display

model = "CHECKPOINT"  #@param ['CHECKPOINT', '游 1F600', '游눤 1F4A5', '游녜 1F441', '游붍 1F98E', '游 1F420', '游붊 1F98B', '游 1F41E', '游돚 1F578', '游볻 1F968', '游꾻 1F384']
model_type = '3 regenerating'  #@param ['1 naive', '2 persistent', '3 regenerating']

#@markdown Shift-click to seed the pattern

if model != 'CHECKPOINT':
  code = model.split(' ')[1]
  emoji = chr(int(code, 16))
  experiment_i = int(model_type.split()[0])-1
  use_pool = (0, 1, 1)[experiment_i]
  damage_n = (0, 0, 3)[experiment_i]
  model_str = get_model(emoji, use_pool=use_pool, damage_n=damage_n, output='json')
else:
  last_checkpoint_fn = sorted(glob.glob('train_log/*.json'))[-1]
  model_str = open(last_checkpoint_fn).read()

data_js = '''
  window.GRAPH_URL = URL.createObjectURL(new Blob([`%s`], {type: 'application/json'}));
'''%(model_str)

display(IPython.display.Javascript(data_js))


IPython.display.HTML('''
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.3.0/dist/tf.min.js"></script>

<canvas id='canvas' style="border: 1px solid black; image-rendering: pixelated;"></canvas>

<script>
  "use strict";
  
  const sleep = (ms)=>new Promise(resolve => setTimeout(resolve, ms));
  
  const parseConsts = model_graph=>{
    const dtypes = {'DT_INT32':['int32', 'intVal', Int32Array],
                    'DT_FLOAT':['float32', 'floatVal', Float32Array]};
    
    const consts = {};
    model_graph.modelTopology.node.filter(n=>n.op=='Const').forEach((node=>{
      const v = node.attr.value.tensor;
      const [dtype, field, arrayType] = dtypes[v.dtype];
      if (!v.tensorShape.dim) {
        consts[node.name] = [tf.scalar(v[field][0], dtype)];
      } else {
        // if there is a 0-length dimension, the exported graph json lacks "size"
        const shape = v.tensorShape.dim.map(d=>(!d.size) ? 0 : parseInt(d.size));
        let arr;
        if (v.tensorContent) {
          const data = atob(v.tensorContent);
          const buf = new Uint8Array(data.length);
          for (var i=0; i<data.length; ++i) {
            buf[i] = data.charCodeAt(i);
          }
          arr = new arrayType(buf.buffer);
        } else {
          const size = shape.reduce((a, b)=>a*b);
          arr = new arrayType(size);
          if (size) {
            arr.fill(v[field][0]);
          }
        }
        consts[node.name] = [tf.tensor(arr, shape, dtype)];
      }
    }));
    return consts;
  }
  
  const run = async ()=>{
    const r = await fetch(GRAPH_URL);
    const consts = parseConsts(await r.json());
    
    const model = await tf.loadGraphModel(GRAPH_URL);
    Object.assign(model.weights, consts);
    
    let seed = new Array(16).fill(0).map((x, i)=>i<3?0:1);
    seed = tf.tensor(seed, [1, 1, 1, 16]);
    
    const D = 96;
    const initState = tf.tidy(()=>{
      const D2 = D/2;
      const a = seed.pad([[0, 0], [D2-1, D2], [D2-1, D2], [0,0]]);
      return a;
    });
    
    const state = tf.variable(initState);
    const [_, h, w, ch] = state.shape;
    
    const damage = (x, y, r)=>{
      tf.tidy(()=>{
        const rx = tf.range(0, w).sub(x).div(r).square().expandDims(0);
        const ry = tf.range(0, h).sub(y).div(r).square().expandDims(1);
        const mask = rx.add(ry).greater(1.0).expandDims(2);
        state.assign(state.mul(mask));
      });
    }
    
    const plantSeed = (x, y)=>{
      const x2 = w-x-seed.shape[2];
      const y2 = h-y-seed.shape[1];
      if (x<0 || x2<0 || y2<0 || y2<0)
        return;
      tf.tidy(()=>{
        const a = seed.pad([[0, 0], [y, y2], [x, x2], [0,0]]);
        state.assign(state.add(a));
      });
    }
    
    const scale = 4;
    
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    canvas.width = w;
    canvas.height = h;
    canvas.style.width = `${w*scale}px`;
    canvas.style.height = `${h*scale}px`;
    
    canvas.onmousedown = e=>{
      const x = Math.floor(e.clientX/scale);
        const y = Math.floor(e.clientY/scale);
        if (e.buttons == 1) {
          if (e.shiftKey) {
            plantSeed(x, y);  
          } else {
            damage(x, y, 8);
          }
        }
    }
    canvas.onmousemove = e=>{
      const x = Math.floor(e.clientX/scale);
      const y = Math.floor(e.clientY/scale);
      if (e.buttons == 1 && !e.shiftKey) {
        damage(x, y, 8);
      }
    }

    function step() {
      tf.tidy(()=>{
        state.assign(model.execute(
            {x:state, fire_rate:tf.tensor(0.5),
            angle:tf.tensor(0.0), step_size:tf.tensor(1.0)}, ['Identity']));
      });
    }

    function render() {
      step();

      const imageData = tf.tidy(()=>{
        const rgba = state.slice([0, 0, 0, 0], [-1, -1, -1, 4]);
        const a = state.slice([0, 0, 0, 3], [-1, -1, -1, 1]);
        const img = tf.tensor(1.0).sub(a).add(rgba).mul(255);
        const rgbaBytes = new Uint8ClampedArray(img.dataSync());
        return new ImageData(rgbaBytes, w, h);
      });
      ctx.putImageData(imageData, 0, 0);

      requestAnimationFrame(render);
    }
    render();
  }
  run();
  
</script>
''')
#@title WebGL Demo

#@markdown This code exports quantized models for the WebGL demo that is used in the article.
#@markdown The demo code can be found at https://github.com/distillpub/post--growing-ca/blob/master/public/ca.js

def pack_layer(weight, bias, outputType=np.uint8):
  in_ch, out_ch = weight.shape
  assert (in_ch%4==0) and (out_ch%4==0) and (bias.shape==(out_ch,))
  weight_scale, bias_scale = 1.0, 1.0
  if outputType == np.uint8:
    weight_scale = 2.0*np.abs(weight).max()
    bias_scale = 2.0*np.abs(bias).max()
    weight = np.round((weight/weight_scale+0.5)*255)
    bias = np.round((bias/bias_scale+0.5)*255)
  packed = np.vstack([weight, bias[None,...]])
  packed = packed.reshape(in_ch+1, out_ch//4, 4)
  packed = outputType(packed)
  packed_b64 = base64.b64encode(packed.tobytes()).decode('ascii')
  return {'data_b64': packed_b64, 'in_ch': in_ch, 'out_ch': out_ch,
          'weight_scale': weight_scale, 'bias_scale': bias_scale,
          'type': outputType.__name__}

def export_ca_to_webgl_demo(ca, outputType=np.uint8):
  # reorder the first layer inputs to meet webgl demo perception layout
  chn = ca.channel_n
  w1 = ca.weights[0][0, 0].numpy()
  w1 = w1.reshape(chn, 3, -1).transpose(1, 0, 2).reshape(3*chn, -1)
  layers = [
      pack_layer(w1, ca.weights[1].numpy(), outputType),
      pack_layer(ca.weights[2][0, 0].numpy(), ca.weights[3].numpy(), outputType)
  ]
  return json.dumps(layers)

with zipfile.ZipFile('webgl_models8.zip', 'w') as zf:
  for e in EMOJI:
    zf.writestr('ex1_%s.json'%e, export_ca_to_webgl_demo(get_model(e, use_pool=0, damage_n=0)))
    run = 1 if e in '游游돚' else 0  # select runs that happen to quantize better
    zf.writestr('ex2_%s.json'%e, export_ca_to_webgl_demo(get_model(e, use_pool=1, damage_n=0, run=run)))
    run = 1 if e in '游붍' else 0    # select runs that happen to quantize better
    zf.writestr('ex3_%s.json'%e, export_ca_to_webgl_demo(get_model(e, use_pool=1, damage_n=3, run=run)))
